<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Being Codest</title><description>A Personal Blog</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Being Codest</title><link>http://localhost:2368/</link></image><generator>Ghost 3.11</generator><lastBuildDate>Sun, 22 Mar 2020 12:24:49 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>VAE in S4TF !!</title><description>&lt;p&gt;It's been 3 months since my first open-source contribution to the S4TF community in the Swift APIs repository and I'm really excited to keep working on this!! S4TF's fantastic high-level API for differentiable programming is what has got me hooked onto this and through this post, I hope on giving&lt;/p&gt;</description><link>http://localhost:2368/vae-in-s4tf/</link><guid isPermaLink="false">5e77424327207e0571be0407</guid><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Sun, 22 Mar 2020 12:24:33 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/swift_tf.png" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/swift_tf.png" alt="VAE in S4TF !!"&gt;&lt;p&gt;It's been 3 months since my first open-source contribution to the S4TF community in the Swift APIs repository and I'm really excited to keep working on this!! S4TF's fantastic high-level API for differentiable programming is what has got me hooked onto this and through this post, I hope on giving back to the community.&lt;/p&gt;&lt;p&gt;Variational Autoencoders (Kingma et al.[1]) have been around for a lot of time and there are many awesome explanations online regarding this. I'll be linking to a few of them, but through this post, I'd like to highlight important Swifty features that S4TF supports and how you can start contributing to this incredible repository!!&lt;/p&gt;&lt;p&gt;The entire code can be found here: &lt;a href="https://github.com/vballoli/swift-models/blob/VAE/Autoencoder/VAE1D/main.swift"&gt;https://github.com/vballoli/swift-models/blob/VAE/Autoencoder/VAE1D/main.swift&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Step 1: Define the autoencoder&lt;/p&gt;&lt;figure class="kg-card kg-code-card"&gt;&lt;pre&gt;&lt;code class="language-Swift"&gt;// Variational Autoencoder
public struct VAE: Layer {
    // Encoder
    public var encoderDense1: Dense&amp;lt;Float&amp;gt;
    public var encoderDense2_1: Dense&amp;lt;Float&amp;gt;
    public var encoderDense2_2: Dense&amp;lt;Float&amp;gt;
    // Decoder
    public var decoderDense1: Dense&amp;lt;Float&amp;gt;
    public var decoderDense2: Dense&amp;lt;Float&amp;gt;

    public init() {
        self.encoderDense1 = Dense&amp;lt;Float&amp;gt;(inputSize: inputDim, outputSize: hiddenDim, activation: relu)
        self.encoderDense2_1 = Dense&amp;lt;Float&amp;gt;(inputSize: hiddenDim, outputSize: latentDim)
        self.encoderDense2_2 = Dense&amp;lt;Float&amp;gt;(inputSize: hiddenDim, outputSize: latentDim)
        
        self.decoderDense1 = Dense&amp;lt;Float&amp;gt;(inputSize: latentDim, outputSize: hiddenDim, activation: relu)
        self.decoderDense2 = Dense&amp;lt;Float&amp;gt;(inputSize: hiddenDim, outputSize: inputDim)
    }

    @differentiable
    public func callAsFunction(_ input: Tensor&amp;lt;Float&amp;gt;) -&amp;gt; [Tensor&amp;lt;Float&amp;gt;] {
        // Encode
        let intermediateInput = encoderDense1(input)
        let mu = encoderDense2_1(intermediateInput)
        let logVar = encoderDense2_2(intermediateInput)

        // Re-parameterization trick
        let std = exp(0.5 * logVar)
        let epsilon = Tensor&amp;lt;Float&amp;gt;(randomNormal: std.shape)
        let z = mu + epsilon * std

        // Decode
        let output = z.sequenced(through: decoderDense1, decoderDense2)
        return [output, mu, logVar]
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;figcaption&gt;Model definition&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;Declare and initialize the model layers&lt;/li&gt;&lt;li&gt;&lt;code&gt;callAsFunction()&lt;/code&gt; lets you 'call ' your struct object - where both input and output must be of type &lt;code&gt;Diiferenitable&lt;/code&gt;. Here, we input an image tensor and return an &lt;code&gt;Array&lt;/code&gt; of Tensors. Pay attention to the return type: [Tensor&amp;lt;Float&amp;gt;]. When &lt;code&gt;import TensorFlow&lt;/code&gt; is executed, it extends &lt;a href="https://www.tensorflow.org/swift/api_docs/Extensions/Array"&gt;Swift's Array implementation&lt;/a&gt; to make it differentiable. That is so cool !! ¬†&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Step 2: Loss function&lt;/p&gt;&lt;figure class="kg-card kg-code-card"&gt;&lt;pre&gt;&lt;code class="language-Swift"&gt;func vaeLossFunction(input: Tensor&amp;lt;Float&amp;gt;, output: Tensor&amp;lt;Float&amp;gt;, mu: Tensor&amp;lt;Float&amp;gt;, logVar: Tensor&amp;lt;Float&amp;gt;) -&amp;gt; Tensor&amp;lt;Float&amp;gt; {
    let crossEntropy = sigmoidCrossEntropy(logits: output, labels: input, reduction: _sum)   
    let klDivergence = -0.5 * (1 + logVar - pow(mu, 2) - exp(logVar)).sum()
    return crossEntropy + klDivergence
}&lt;/code&gt;&lt;/pre&gt;&lt;figcaption&gt;Loss function&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This is relatively simple. The loss function's value must be scalarized, and since all the intermediate operations and functions are differentiable, the entire function becomes differentiable.&lt;/p&gt;&lt;p&gt;Step 3: Train !!&lt;/p&gt;&lt;p&gt;Load dataset, set hyperparameters, that's a go on training !! &lt;/p&gt;&lt;figure class="kg-card kg-code-card"&gt;&lt;pre&gt;&lt;code class="language-Swift"&gt;for batch in trainingShuffled.batched(batchSize) {
	let x = batch.data
	let ùõÅmodel = TensorFlow.gradient(at: vae) { vae -&amp;gt; Tensor&amp;lt;Float&amp;gt; in
	let outputs = vae(x)
	let output = outputs[0]
	let mu = outputs[1]
	let logVar  = outputs[2]
	return vaeLossFunction(input: x, output: output, mu: mu, logVar: logVar)
}

optimizer.update(&amp;amp;vae, along: ùõÅmodel)&lt;/code&gt;&lt;/pre&gt;&lt;figcaption&gt;Sample training&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And that's it!! Training a 1 dimensional AutoEncoder is that simple in Swift for TensorFlow. More such tutorials can be found on their official website - &lt;a href="https://www.tensorflow.org/swift/tutorials/model_training_walkthrough"&gt;S4TF&lt;/a&gt;. You must now be wondering, "That's awesome! How do I start contributing !?". Well, it's pretty intuitive:&lt;/p&gt;&lt;p&gt;1. Write your code(duh)&lt;/p&gt;&lt;p&gt;2. Tell Swift to compile your code by adding you main directory's path to &lt;a href="https://github.com/vballoli/swift-models/blob/VAE/Package.swift"&gt;Package.swift&lt;/a&gt; - based on what you've implemented, either a simple run by adding in the targets section - e.g. &lt;code&gt;.target(name: "VariationalAutoencoder1D", dependencies: ["Datasets", "ModelSupport"], path: "Autoencoder/VAE1D")&lt;/code&gt; in the targets array and run it as &lt;code&gt;swift run -c release VAE1D&lt;/code&gt; or ¬†if it is a library like code, add it to products - e.g. &lt;code&gt;.library(name: "Name", targets: ["Name"])&lt;/code&gt; and their corresponding targets &lt;code&gt;.target(name: "Name", path: "Name/Name2/Name3")&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;3. Run &lt;code&gt;swift build&lt;/code&gt; and you're all set.&lt;/p&gt;&lt;p&gt;4. Send a pull request and onto next ones !!&lt;/p&gt;&lt;p&gt;Looking forward to comments on this and stay tuned for more.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;[1] Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." &lt;em&gt;arXiv preprint arXiv:1312.6114&lt;/em&gt; (2013).&lt;/p&gt;</content:encoded></item><item><title>On Research...</title><description>Although I'm a toddler in the world of wise, old researchers, there is one thing I wish I knew when I was starting out. </description><link>http://localhost:2368/on-research/</link><guid isPermaLink="false">5e710fd5f68c8e69a047aea7</guid><category>Research</category><category>Thoughts</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Sat, 05 Oct 2019 18:29:20 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/michael-dziedzic-1bjsASjhfkE-unsplash.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/michael-dziedzic-1bjsASjhfkE-unsplash.jpg" alt="On Research..."&gt;&lt;p&gt;As an aspiring researcher, I'm starting to love research, but lately it feels like it is a fibre-optic cable : one thing on the outside, something else on the inside (nice pun, I know). Although I'm a toddler in the world of wise, old researchers, there is one thing I wish I knew when I was starting out. &lt;/p&gt;&lt;p&gt;Skimming through the papers on Arxiv, specially in areas with Deep Learning, there's some charm attached to almost all papers which fascinated me in the initial phases of my slow paced catch with the field. As I started to really get an idea of these papers, my next obvious step was to go through the code to get a clear perspective on proposals and results. While some papers really blew my mind, there was something obvious with most papers - they were barely reproducible. To clarify, most of the papers did have a &lt;em&gt;part of the code&lt;/em&gt; on their Github repository, ¬†only to prove their results, but did not exhibit any actual code as to how those results were obtained, which in my view, defeats the purpose of actually publishing such ground breaking work. I believe any and all code that was involved in the whole research methodology adopted by the authors and researchers should be published and open to public and public scrutiny. To my surprise, NeurIPS, the cr√®me de la cr√®me of AI conferences, has a challenge dedicated to reproducing the papers published. Not that I'm criticising the challenge, I actually love the challenge, but the very fact that the challenge exists is an indication of something that is clearly off in this area's research and researchers.&lt;/p&gt;&lt;p&gt;While I understand it's not all black and white in publishing the entire code, I'm fairly confident that there aren't many reasons to not publish the entire code unless it involves proprietary stuff, which then should probably be specifically mentioned and then kept under the wraps. Although I can only comment on the AI part of Computer Science publications, I'm certain it's probably true with many other parts. There should probably be a gold standard like NeurIPS for just reproducibility and modelling any and all such code for any publication so that the better part of the time can be spent prototyping. I think docker, specific code guidelines adopted from the open source communities and a hosting engine specifically curated for this could be a possible solution, but as a curious undergrad and aspiring researcher, this is probably my only complain. I'm hoping there are advances in this, because as this field evolves(which is exponential really), revolutionary papers and ideas with code behind seal is like a locked treasure with a lost key, practically useless.&lt;/p&gt;</content:encoded></item><item><title>Thoughts on the AI podcasts hosted by Lex Fridman</title><description>&lt;h3 id="recently-i-ve-taken-up-listening-to-podcasts-on-apple-podcasts-on-my-40-minute-commute-to-the-office-where-i-m-pursuing-a-corporate-thesis-as-a-part-of-my-curriculum-and-this-podcast-the-lex-fridman-s-podcasts-on-artificial-intelligence-caught-my-attention-"&gt;Recently, I‚Äôve taken up listening to podcasts on Apple Podcasts on my 40 minute commute to the office where I‚Äôm pursuing a corporate thesis as a part of my curriculum and this podcast, The &lt;a href="https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4" rel="noreferrer nofollow noopener"&gt;Lex Fridman‚Äôs podcasts on Artificial Intelligence&lt;/a&gt; caught my attention.&lt;/h3&gt;&lt;p&gt; I particularly like this&lt;/p&gt;</description><link>http://localhost:2368/thoughts-on-the-ai-podcasts-hosted-by-lex-fridman-2/</link><guid isPermaLink="false">5e710fd5f68c8e69a047aea6</guid><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Tue, 16 Jul 2019 18:07:06 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/download_o.jpg" medium="image"/><content:encoded>&lt;h3 id="recently-i-ve-taken-up-listening-to-podcasts-on-apple-podcasts-on-my-40-minute-commute-to-the-office-where-i-m-pursuing-a-corporate-thesis-as-a-part-of-my-curriculum-and-this-podcast-the-lex-fridman-s-podcasts-on-artificial-intelligence-caught-my-attention-"&gt;Recently, I‚Äôve taken up listening to podcasts on Apple Podcasts on my 40 minute commute to the office where I‚Äôm pursuing a corporate thesis as a part of my curriculum and this podcast, The &lt;a href="https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4" rel="noreferrer nofollow noopener"&gt;Lex Fridman‚Äôs podcasts on Artificial Intelligence&lt;/a&gt; caught my attention.&lt;/h3&gt;&lt;img src="http://localhost:2368/content/images/2020/03/download_o.jpg" alt="Thoughts on the AI podcasts hosted by Lex Fridman"&gt;&lt;p&gt; I particularly like this podcast is because the host, Lex Fridman explores the minds of the best researchers in CS by talking about their take on the world of technology, both scientifically and philosophically. The guests on this podcasts are people who‚Äôs papers and blog posts I‚Äôve been reading and following and their views are inspiring and also conflicting because that makes me question myself on what I think about the very same topics. Although I haven‚Äôt listened to all of them, some of them have impacted me deeply on the way I perceive the world and the world of tech:&lt;/p&gt;&lt;h3 id="dr-greg-brockman-openai-founder-founder-stripe-"&gt;Dr. Greg Brockman, OpenAI(founder, founder Stripe)&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/bIrEM2FbOLU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;h3 id="dr-rosalind-picard-mit"&gt;Dr. Rosalind Picard, MIT&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/kq0VO1FqE6I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;h3 id="dr-chris-lattner-google-former-apple-tesla-"&gt;Dr. Chris Lattner, Google (former Apple, Tesla)&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/yCd3CzGSte8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;h3 id="dr-ian-goodfellow-apple-author-of-gan-"&gt;Dr. Ian Goodfellow, Apple (author of GAN)&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/Z6rxFNMGdn0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;h3 id="dr-oriol-vinyals-alphastar-deepmind-former-google-brain-"&gt;Dr. Oriol Vinyals, AlphaStar - Deepmind (former Google Brain)&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/Kedt2or9xlo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;h3 id="elon-musk"&gt;Elon Musk&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/videoseries?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;h3 id="kai-fu-lee-author-of-ai-superpower-"&gt;Kai-Fu Lee (Author of AI Superpower)&lt;/h3&gt;&lt;figure class="kg-card kg-embed-card"&gt;&lt;iframe width="480" height="270" src="https://www.youtube.com/embed/cQ48rP_Rs4g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;p&gt;All these people have talked about AGI (Dr. Brockman), Affective computing (Dr. Picard), various aspects of deep learning - ¬†progress, future, limits, opportunities, advancements, etc. These topics are particularly interesting because no one discusses them in scientific papers and various communities which ideally should be important so that everyone has a fair idea about where the field, it‚Äôs current and potential impact irrespective of their profession. I particularly liked the Elon Musk podcast because I really liked the confidence behind every statement and for the most of it, I did agree - specially the one segment which involved how the people and governments see regulating self-driving cars and why they are required. At one particular instance Elon Musk illustrates his point on human's vigilance in the self-driving car's performance:&lt;/p&gt;&lt;blockquote&gt;You wouldn't want some human to operate an elevator through levers, you would rather have the elevator stop automatically through buttons. That's obviously safer.... ¬†In the future, people will be like "I can't believe humans were allowed to drive a 2 ton death machine by themselves." &lt;/blockquote&gt;&lt;p&gt;These words seem wrong and almost arrogant, but if you really think about it, it's true! All of these episodes have a similar tone and these have exposed me to new ideas and paradigms. I do recommend watching/listening to all of these while I finish listening to the remaining episodes. &lt;/p&gt;</content:encoded></item><item><title>Making some 'shroom !</title><description>&lt;p&gt;While I was studying, rather exploring more about Reinforcement Learning, I suddenly felt the urge to cook and I don't know why, the first thing that struck me was mushroom. Mom was surprised and glad at the same time, can't say which one though but she suggested I make the&lt;/p&gt;</description><link>http://localhost:2368/making-some-sh-room/</link><guid isPermaLink="false">5e710fd5f68c8e69a047aea5</guid><category>cooking</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Fri, 24 May 2019 17:55:27 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/mario_o.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/mario_o.jpg" alt="Making some 'shroom !"&gt;&lt;p&gt;While I was studying, rather exploring more about Reinforcement Learning, I suddenly felt the urge to cook and I don't know why, the first thing that struck me was mushroom. Mom was surprised and glad at the same time, can't say which one though but she suggested I make the "Shahi Mushroom" curry !&lt;/p&gt;&lt;p&gt;It wasn't tough to be honest - but it did involve a lot of spices which I've never used. I searched for the recipe and the first link was this from &lt;a href="https://food.ndtv.com/recipe-shahi-mushroom-233314"&gt;NDTV&lt;/a&gt; - so I followed it, but not exactly. Mom was monitoring me and when I showed her the recipe - she condensed a 10 step procedure into 3 with a simple philosophy: mix everything in the beginning to ease things. After that, it was all grinding, frying and mixing. In the end, it did look like this: &lt;/p&gt;&lt;figure class="kg-card kg-image-card"&gt;&lt;img src="http://localhost:2368/content/images/2020/03/IMG_2712_o.JPG" class="kg-image" alt="Making some 'shroom !"&gt;&lt;/figure&gt;&lt;p&gt;It was good - but that's a biased opinion üòÑ. Looking forward to learn more, or rather survive after cooking them I suppose. &lt;/p&gt;</content:encoded></item><item><title>Keras like Neural Network</title><description>Assignment 3: Implement an ANN - I went all Kerasy !!!</description><link>http://localhost:2368/keras-like-neural-network/</link><guid isPermaLink="false">5e710fd5f68c8e69a047aea4</guid><category>Projects</category><category>Neural Network</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Thu, 25 Apr 2019 18:17:05 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/neural_network-1522356325493_o.jpeg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/neural_network-1522356325493_o.jpeg" alt="Keras like Neural Network"&gt;&lt;p&gt;Assignment 3: Implement an ANN - I went all Kerasy !!!&lt;/p&gt;&lt;p&gt;This semester I'm enrolled in the BITS F464 Machine Learning course and the third assignment was to implement a neural network. The concepts were pretty clear and so was the implementation. But I wanted it to take it a level up, so I decided I wanted to do it in a ¬†sequential fashion like Keras. I felt the effort for this deserved a post!&lt;/p&gt;&lt;p&gt;First off, I started with implementing it in C++ so that I could brush up my basics in it and learn more about it and it seemed the perfect way to do it. Although the Bishop's mathematical explanation was clear, I had to derive the equations myself to get a clear picture of how the code should look like. So I did it: wrote the code for a "neuron" class and then the neural network class: the problem was a clang error popped up and I couldn't find any solution on the good ol' StackOverflow. I now had a hard deadline approaching ...&lt;/p&gt;&lt;p&gt;I immediately switched to Python - something I'm very comfortable with: the core code was intact but overall it was a pretty shabby implementation, but I feel pretty satisfied in the end. If I ever find the time, I'll definitely clean the code and structure it better to give it a good finishing.&lt;/p&gt;&lt;p&gt;Do check &lt;/p&gt;&lt;blockquote&gt; &lt;a href="https://github.com/vbbphc/neural-net/tree/master/python"&gt;https://github.com/vbbphc/neural-net/tree/master/python&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;and star it if you like ! &lt;/p&gt;</content:encoded></item><item><title>Conference calls !</title><description>My experience presenting our paper titled "Video Streaming using Scalable Video Coding over Opportunistic Networks" at WiSPNET 2019.</description><link>http://localhost:2368/conference-calls/</link><guid isPermaLink="false">5e710fd5f68c8e69a047aea3</guid><category>Conference</category><category>Travel</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Sat, 30 Mar 2019 20:16:13 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/andrei-stratu-576842-unsplash_o.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/andrei-stratu-576842-unsplash_o.jpg" alt="Conference calls !"&gt;&lt;p&gt;I recently published a research paper titled "Video Streaming using Scalable Video Coding over Opportunistic Networks" under my mentor and former professor at CS&amp;amp;IS Dept., BITS Pilani Hyderabad, ¬†Mr. Abhishek Thakur and &lt;a href="http://localhost:2368/conference-calls/www.arnavdhamija.com"&gt;Arnav Dhamija&lt;/a&gt; (one of the best people I've met in college), and got an opportunity to present it at an IEEE conference &lt;a href="http://wispnet2019.org"&gt;WiSPNET 2019&lt;/a&gt; at SSN Engineering College, Chennai. &lt;/p&gt;&lt;p&gt;I enjoyed the travel as much as I enjoyed while presenting the paper. This being my first time travelling alone in an unknown city, I had a constant fear of being stuck in the middle of nowhere and missing my presentation. But, I only have praise for the public transport in Chennai. I could reach anywhere in the public buses and never felt the need to book a cab and the people around were quite friendly even though the language barrier was an issue. &lt;/p&gt;&lt;p&gt;I reached quite comfortably after getting down at Egmore, Chennai and switched 3 buses to reach the college. After checking in with the organizers, I quickly changed and hurried back so that I could attend the talks scheduled for the rest of the day. Although the talks were really good, I could only understand a part of Dr. Tolga Duman and Dr. Giovanni Schembra's talks. In Dr. Schembra's talk about "The Key enablers of 5G ecosystem", he talked about Network Softwarization, Virtualization, Resource management and orchestration extensively which was a pretty good introduction for the 5G architecture I suppose. The one thing that struck me was his conclusion of the talk where he says everyone must learn about Artificial Intelligence irrespective of their field of study, supporting it by claiming that the 6G network architecture would be fully AI controlled.&lt;/p&gt;&lt;p&gt;Fast forward to the presentation, it was T-5 minutes and I had a terrible headache. I tried staying calm to avoid any panic. At the beginning of my presentation, there was a mishap - the &lt;a href="https://www.youtube.com/watch?v=0gCMIiJdYPQ"&gt;video&lt;/a&gt; I had embedded didn't start automatically as it should have and I ended up explaining it manually using a still image of the inter-planetary illustration in the image which was so simply shown in the video. Everything was smooth later on and felt good at the end of presentation, although there weren't many people in the audience. Ending my day with a decent dinner and an episode of Billions, I hopped onto the flight back scheduled in the afternoon next day. The whole week was a roller coaster ride from preparing the presentation in the last 3 days to travelling and presenting at the conference and ending with concerts of Pineapple Express and Vishal-Shekhar - couldn't have asked for more!&lt;/p&gt;</content:encoded></item><item><title>Decentralised Chat</title><description>&lt;p&gt;A decentralised work in progress.&lt;/p&gt;&lt;p&gt;A couple of months ago while binge watching YouTube, I came across a very interesting TechCrunch interview with &lt;a href="https://www.youtube.com/watch?v=WSN5BaCzsbo"&gt;Vitalik Buterin&lt;/a&gt; (do watch it, it's worth the time), co-founder of Ethereum which got me curious about the Ethereum and crytpocurrencies in general. After digging a little&lt;/p&gt;</description><link>http://localhost:2368/decentralised-chat/</link><guid isPermaLink="false">5e710fd5f68c8e69a047aea0</guid><category>Projects</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Sat, 03 Nov 2018 20:33:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/thought-catalog-580700-unsplash_o.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/thought-catalog-580700-unsplash_o.jpg" alt="Decentralised Chat"&gt;&lt;p&gt;A decentralised work in progress.&lt;/p&gt;&lt;p&gt;A couple of months ago while binge watching YouTube, I came across a very interesting TechCrunch interview with &lt;a href="https://www.youtube.com/watch?v=WSN5BaCzsbo"&gt;Vitalik Buterin&lt;/a&gt; (do watch it, it's worth the time), co-founder of Ethereum which got me curious about the Ethereum and crytpocurrencies in general. After digging a little into Ethereum, I found &lt;a href="https://github.com/ethereum/solidity"&gt;Solidity&lt;/a&gt;. Solidity is a statically typed language developed by the same people who've developed Ethereum, a turing complete blockchain, essentially meaning any problem can be solved assuming infinite memory, which was lacking in Bitcoin. Impressive!&lt;/p&gt;&lt;p&gt;Coincidentally, I enrolled in BITS F463 Cryptography course as a disciplinary elective which had an evaluated project component. So, I decided to use solidity and build an app. That's how the idea of decentralised chat started. You can find the project on github:&lt;/p&gt;&lt;p&gt;&lt;code&gt;https://github.com/vbbphc/decentralised-chat&lt;/code&gt;&lt;/p&gt;&lt;p&gt;I decided to use Node.js since it was easy to work with and I found many resources and documentation in nodejs than any other framework. My first commit was a simple smart contract having only one functionality : sending a message using Truffle. Although it was only a few lines of code, it took me quite a while to understand it given there are very few beginner-friendly resources on the web about building DApps in solidity. This was going pretty well until I took a brief break for this project since I had many other assignments, projects and tests to complete and prepare for, leaving me exhausted at the end of the day and this project unattended.&lt;/p&gt;&lt;p&gt;The second commit was rushed since my midterm report submission was closing by and I had very little progress. This commit was mostly Javascript code for writing test for the smart contract, the chat UI and contract calls using bootstrap and Web3 frameworks. The concept of Web3 simply amazes me, although I still don't understand it completely. After working for 6 hours straight with programming languages in which I'm a complete beginner, the results were very satisfying. Using the combination of Truffle, Ganache, MetaMask - which I'd coin the term the Solidity-Dev-Trio, I built a simple, working chat room. I'll update about my progress on this later, given I have my mid term submission tomorrow. I hope I find time for adding more functionality to this, given it has a very crappy UI. Do try it out by forking the repository and if you like it, star it!&lt;/p&gt;</content:encoded></item><item><title>iOS 12 and macOS Mojave - Best Duet Ever!</title><description>&lt;p&gt;Amidst all the gates ensuing Apple now, all the focus has gone away from the best release Apple has ever made - not the iPhone XS or XS Max (maybe the Apple Watch 4, but too pricey) , but iOS 12 and macOS Mojave ! I recently got an iPhone SE after&lt;/p&gt;</description><link>http://localhost:2368/ios-12-and-macos-mojave-best-duet-ever/</link><guid isPermaLink="false">5e710fd5f68c8e69a047ae9f</guid><category>Tech</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Mon, 01 Oct 2018 20:32:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/ibrahim-rifath-720971-unsplash_o.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/ibrahim-rifath-720971-unsplash_o.jpg" alt="iOS 12 and macOS Mojave - Best Duet Ever!"&gt;&lt;p&gt;Amidst all the gates ensuing Apple now, all the focus has gone away from the best release Apple has ever made - not the iPhone XS or XS Max (maybe the Apple Watch 4, but too pricey) , but iOS 12 and macOS Mojave ! I recently got an iPhone SE after my old Moto G4 play started to lag and cause issues, thus moving into the Apple's walled garden completely. I'm now running the latest software release of MacOS on a 3 year old MacBook Air (Early 2015) and iOS a 2 year old iPhone SE (since release) and I've never been happier !&lt;/p&gt;&lt;p&gt;After shifting from Android to iOS, I'll admit that it felt a bit weird but that feeling only lasted so long. Hours into my new phone, I realised how bad the experience on Android was. I got iPhone SE because of it's small screen size and the powerful A9 chip. Every Android phone now has a notch, screen size not less than 5.7 inches, a very fragile screen and glass back - all the factors for a high-maintenance phone and that's why I went with iPhone SE (also, it was pretty cheap).&lt;/p&gt;&lt;p&gt;iOS 12 has given me the smoothest experience on a phone, although it has the first generation TouchID sensor who's lag is a bit noticeable. Now that Mojave is released, the sync between these devices is amazing. The continuity camera feature where photos can directly be imported on to the Mac from the iPhone is the best and taking screenshots on the MacBook now has a whole new experience. The only imperfection with the current iOS is lack of dark mode, hopefully they release it sometime soon making the best device combination ever!&lt;/p&gt;&lt;p&gt;The Mojave release proves why the old Apple products are still worth it in today's high-priced Apple garden, because each update of the macOS has made me happier(from Sierra to High Sierra to now, Mojave), unlike Windows which is a disaster in itself. My MacBook still work flawlessly, my development experience has been crisp and I love the new dark mode ! The only thing I had to worry about the update is the HAXM driver on the new OS, but it works just fine. Hopefully Apple continues the same trend in their software development, although I'd want to see their response for this year's "gates" they're facing for the newly released iPhones - should be hilarious, but is it worth the wait ?&lt;/p&gt;</content:encoded></item><item><title>Smart Campus App 2.0 - Lessons learnt</title><description>Smart Campus is a student-innovation project at BITS Pilani, Hyderabad Campus with Cashless transaction system as the first project.</description><link>http://localhost:2368/smart-campus-app-2-0-lessons-learnt/</link><guid isPermaLink="false">5e710fd5f68c8e69a047ae9e</guid><category>Smart Campus</category><category>College</category><dc:creator>Vaibhav Balloli</dc:creator><pubDate>Sun, 09 Sep 2018 20:27:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2020/03/unnamed_o-1.png" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2020/03/unnamed_o-1.png" alt="Smart Campus App 2.0 - Lessons learnt"&gt;&lt;p&gt;A little bit of background: Smart Campus was a student initiative taken up at BPHC a few years ago. It was aimed to better the life on campus through innovative tech. As a part of the Android app development team, we made an app that came into production in 2018. I mainly handled a part of the back end of the app and mostly UI/UX, nothing major back then but, I learned a ton from my senior whom I was working with. Fast forward now, the whole app needed a redesign to accommodate many more features and additions to it and I had to do it alone...&lt;/p&gt;&lt;p&gt;Redesigning the whole app took me 5 days and it felt like a code sprint for me and here‚Äôs when I truly realised a few things about importance of things that seem unnecessary but play a major role&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Never Use a Library in it‚Äôs alpha and Canary Studio is bad&lt;/strong&gt;&lt;/strong&gt;: While going through the updated material guidelines from Google this year released during the I/O ‚Äô18, I was excited to adopt the new components like Chips, updated Material Buttons.I wasn‚Äôt ready to refactor all of existing code to AndroidX, so I decided to use the support library for now since it was just a simple one line gradle addition. Here‚Äôs when the mess started ‚Äì this library required the canary version of Android Studio, which was a total mess on my Mac. It was consuming lot of RAM, and runs slower than ever even after limiting its maximum consumption. I ended up restarting studio every 2 hours. The XML attributes in the documentation were hardly present and ended up giving runtime issues all the time. Since they were trivial attributes, I prefer using XML over Java. Also, some attributes that were present, wasn‚Äôt being picked up by studio's autocomplete, reason why I'm so lazy. The only solution was to compile, run and hope it doesn't crash. Chips were a disaster during implementation ‚Äì They weren‚Äôt working as expected and the latest version of support library then introduced another attribute which failed to compile every button and chip I used from this library.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Maintaining code quality, consistency and readability&lt;/strong&gt;&lt;/strong&gt;: I always knew how important this was, but never had an epiphany for it until I perused the previous code. All those doc strings and methods were so precise and clean, I never called my senior up to ask what's or why's of any method.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;strong&gt;RxJava and Realm notifications for the win&lt;/strong&gt;&lt;/strong&gt;: The earlier version of the app only used RxJava to chain request and handle the request errors better. Now, after going through an illustrative tutorial of RxJava here, I realised it‚Äôs potential and since realm notifications were primitively using them, I decided I can learn more about these and these very concepts decreased a lot of my code.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Google‚Äôs developer documentation is good, maybe a bit too much&lt;/strong&gt;&lt;/strong&gt;: Integrating Google‚Äôs OAuth2 sign in was probably the biggest challenge. Their beginner documentation is just right, it is only after that that there are so many in-depth implementations of it from Google themselves that I couldn‚Äôt decide which one to use. It was only after a fortnight or so I came across AccountManager and Credentials API that would have made things easier. Not a single source pointed at these.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;strong&gt;Android Studio‚Äôs debugger is the best tool on the IDE&lt;/strong&gt;&lt;/strong&gt;: The debugging feature is a life saver and probably the most underrated feature. There was a massive reduction of timber lines in my code and took that as a positive outcome of using the debugger so frequently.&lt;/li&gt;&lt;/ol&gt;</content:encoded></item></channel></rss>